{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoYXe1m31qYrdz9oCnNNbl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guifav/curso-ia-aplicada/blob/main/Previsibilidade_de_Churn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3U_KRQ3-hEi"
      },
      "outputs": [],
      "source": [
        "# Importando as bibliotecas necessárias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n",
        "import shap\n",
        "from google.colab import files, userdata\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from IPython.display import HTML, display\n",
        "import json\n",
        "import openai\n",
        "import io\n",
        "from datetime import datetime\n",
        "\n",
        "# Configuração para visualizações\n",
        "plt.style.use('fivethirtyeight')\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "# Configuração da API da OpenAI\n",
        "try:\n",
        "    openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "    openai_enabled = True\n",
        "    print(\"Chave da API OpenAI carregada com sucesso!\")\n",
        "except Exception as e:\n",
        "    print(f\"Erro ao carregar a chave da API OpenAI: {e}\")\n",
        "    print(\"O relatório por IA não estará disponível.\")\n",
        "    openai_enabled = False\n",
        "\n",
        "# Função para upload de dados de treinamento e previsão\n",
        "def upload_data():\n",
        "    data_dict = {}\n",
        "\n",
        "    print(\"\\n==== UPLOAD DE DADOS DE TREINAMENTO ====\")\n",
        "    print(\"Por favor, faça o upload do arquivo CSV com os dados históricos dos clientes.\")\n",
        "    print(\"Este arquivo deve conter uma coluna 'Churn' (0 = não cancelou, 1 = cancelou).\")\n",
        "\n",
        "    uploaded = files.upload()\n",
        "    for fn in uploaded.keys():\n",
        "        print(f'Arquivo \"{fn}\" foi carregado com sucesso.')\n",
        "        data_dict['train'] = pd.read_csv(fn)\n",
        "\n",
        "    print(\"\\n==== UPLOAD DE DADOS PARA PREVISÃO ====\")\n",
        "    print(\"Agora, faça o upload do arquivo CSV com os novos clientes para previsão.\")\n",
        "    print(\"Este arquivo deve ter o mesmo formato que o anterior, mas não precisa ter a coluna 'Churn'.\")\n",
        "    print(\"Se a coluna 'Churn' estiver presente, ela será ignorada para previsão.\")\n",
        "\n",
        "    uploaded = files.upload()\n",
        "    for fn in uploaded.keys():\n",
        "        print(f'Arquivo \"{fn}\" foi carregado com sucesso.')\n",
        "        data_dict['new'] = pd.read_csv(fn)\n",
        "\n",
        "    return data_dict\n",
        "\n",
        "# Função para análise exploratória rápida dos dados de treinamento\n",
        "def analyze_training_data(df_train):\n",
        "    print(\"\\n==== ANÁLISE RÁPIDA DOS DADOS DE TREINAMENTO ====\")\n",
        "\n",
        "    # Informações gerais\n",
        "    print(f\"Total de registros: {df_train.shape[0]}\")\n",
        "    print(f\"Total de características: {df_train.shape[1]}\")\n",
        "\n",
        "    # Taxa de churn\n",
        "    if 'Churn' in df_train.columns:\n",
        "        churn_rate = df_train['Churn'].mean() * 100\n",
        "        print(f\"\\nTaxa de Churn nos dados de treinamento: {churn_rate:.2f}%\")\n",
        "\n",
        "        # Gráfico da distribuição de churn\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        sns.countplot(x='Churn', data=df_train)\n",
        "        plt.title('Distribuição de Churn nos Dados de Treinamento')\n",
        "        plt.xlabel('Churn (0 = Não, 1 = Sim)')\n",
        "        plt.ylabel('Número de Clientes')\n",
        "        plt.show()\n",
        "\n",
        "    # Verificando valores ausentes\n",
        "    missing_values = df_train.isnull().sum().sum()\n",
        "    if missing_values > 0:\n",
        "        print(f\"\\nAtenção: {missing_values} valores ausentes encontrados nos dados de treinamento.\")\n",
        "        missing_cols = df_train.columns[df_train.isnull().any()].tolist()\n",
        "        print(f\"Colunas com valores ausentes: {missing_cols}\")\n",
        "\n",
        "        # Preenchendo valores ausentes\n",
        "        for col in missing_cols:\n",
        "            if df_train[col].dtype in ['int64', 'float64']:\n",
        "                df_train[col].fillna(df_train[col].median(), inplace=True)\n",
        "            else:\n",
        "                df_train[col].fillna(df_train[col].mode()[0], inplace=True)\n",
        "        print(\"Valores ausentes foram preenchidos automaticamente.\")\n",
        "\n",
        "    return df_train\n",
        "\n",
        "# Função para pré-processamento dos dados de treinamento e previsão\n",
        "def preprocess_data(data_dict):\n",
        "    print(\"\\n==== PRÉ-PROCESSAMENTO DE DADOS ====\")\n",
        "\n",
        "    df_train = data_dict['train']\n",
        "    df_new = data_dict['new']\n",
        "\n",
        "    # Verifica se a coluna 'Churn' existe no conjunto de treinamento\n",
        "    if 'Churn' not in df_train.columns:\n",
        "        print(\"ERRO: A coluna 'Churn' não foi encontrada nos dados de treinamento.\")\n",
        "        return None\n",
        "\n",
        "    # Separando features e target no conjunto de treinamento\n",
        "    X_train = df_train.drop('Churn', axis=1)\n",
        "    y_train = df_train['Churn']\n",
        "\n",
        "    # Para o conjunto de novos dados\n",
        "    if 'Churn' in df_new.columns:\n",
        "        print(\"A coluna 'Churn' foi encontrada nos dados de previsão, mas será ignorada.\")\n",
        "        X_new = df_new.drop('Churn', axis=1)\n",
        "    else:\n",
        "        X_new = df_new.copy()\n",
        "\n",
        "    # Verificando se as colunas são compatíveis\n",
        "    train_cols = set(X_train.columns)\n",
        "    new_cols = set(X_new.columns)\n",
        "\n",
        "    if train_cols != new_cols:\n",
        "        missing_in_new = train_cols - new_cols\n",
        "        extra_in_new = new_cols - train_cols\n",
        "\n",
        "        if missing_in_new:\n",
        "            print(f\"AVISO: As seguintes colunas estão no conjunto de treinamento mas não nos novos dados: {missing_in_new}\")\n",
        "            print(\"Estas colunas serão preenchidas com valores padrão (0 para numéricas, 'unknown' para categóricas).\")\n",
        "\n",
        "            for col in missing_in_new:\n",
        "                if df_train[col].dtype in ['int64', 'float64']:\n",
        "                    X_new[col] = 0\n",
        "                else:\n",
        "                    X_new[col] = 'unknown'\n",
        "\n",
        "        if extra_in_new:\n",
        "            print(f\"AVISO: As seguintes colunas estão nos novos dados mas não no conjunto de treinamento: {extra_in_new}\")\n",
        "            print(\"Estas colunas serão removidas para a previsão.\")\n",
        "\n",
        "            X_new = X_new.drop(columns=list(extra_in_new))\n",
        "\n",
        "    # Verificando valores ausentes nos novos dados\n",
        "    missing_values_new = X_new.isnull().sum().sum()\n",
        "    if missing_values_new > 0:\n",
        "        print(f\"\\nAtenção: {missing_values_new} valores ausentes encontrados nos dados de previsão.\")\n",
        "        missing_cols = X_new.columns[X_new.isnull().any()].tolist()\n",
        "        print(f\"Colunas com valores ausentes: {missing_cols}\")\n",
        "\n",
        "        # Preenchendo valores ausentes\n",
        "        for col in missing_cols:\n",
        "            if X_new[col].dtype in ['int64', 'float64']:\n",
        "                X_new[col].fillna(X_train[col].median(), inplace=True)\n",
        "            else:\n",
        "                X_new[col].fillna(X_train[col].mode()[0], inplace=True)\n",
        "        print(\"Valores ausentes foram preenchidos automaticamente.\")\n",
        "\n",
        "    # Tratando colunas categóricas com one-hot encoding\n",
        "    X_train_encoded = pd.get_dummies(X_train, drop_first=True)\n",
        "    X_new_encoded = pd.get_dummies(X_new, drop_first=True)\n",
        "\n",
        "    # Garantindo que X_new_encoded tenha as mesmas colunas que X_train_encoded\n",
        "    for col in X_train_encoded.columns:\n",
        "        if col not in X_new_encoded.columns:\n",
        "            X_new_encoded[col] = 0\n",
        "\n",
        "    # Removendo colunas extras em X_new_encoded\n",
        "    extra_cols = [col for col in X_new_encoded.columns if col not in X_train_encoded.columns]\n",
        "    if extra_cols:\n",
        "        X_new_encoded = X_new_encoded.drop(columns=extra_cols)\n",
        "\n",
        "    # Reordenando colunas para garantir a mesma ordem\n",
        "    X_new_encoded = X_new_encoded[X_train_encoded.columns]\n",
        "\n",
        "    # Normalização dos dados\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
        "    X_new_scaled = scaler.transform(X_new_encoded)\n",
        "\n",
        "    print(f\"Dimensões do conjunto de treinamento: {X_train_encoded.shape}\")\n",
        "    print(f\"Dimensões do conjunto de previsão: {X_new_encoded.shape}\")\n",
        "\n",
        "    return {\n",
        "        'X_train': X_train_encoded,\n",
        "        'X_new': X_new_encoded,\n",
        "        'y_train': y_train,\n",
        "        'X_train_scaled': X_train_scaled,\n",
        "        'X_new_scaled': X_new_scaled,\n",
        "        'feature_names': X_train_encoded.columns,\n",
        "        'scaler': scaler,\n",
        "        'original_df_new': df_new  # Mantendo o DataFrame original para referência\n",
        "    }\n",
        "\n",
        "# Função para treinamento do modelo e previsão nos novos dados\n",
        "def train_and_predict(processed_data):\n",
        "    print(\"\\n==== TREINAMENTO DO MODELO E PREVISÃO ====\")\n",
        "\n",
        "    # Inicialização e treinamento do modelo\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(processed_data['X_train_scaled'], processed_data['y_train'])\n",
        "\n",
        "    # Previsão nos novos dados\n",
        "    y_pred_proba = model.predict_proba(processed_data['X_new_scaled'])[:, 1]\n",
        "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
        "\n",
        "    print(\"Modelo treinado com sucesso e aplicado aos novos dados.\")\n",
        "\n",
        "    # Importância das características\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': processed_data['feature_names'],\n",
        "        'Importance': model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    print(\"\\nImportância das características (top 10):\")\n",
        "    print(feature_importance.head(10))\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
        "    plt.title('Importância das Características')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'feature_importance': feature_importance,\n",
        "        'y_pred_proba': y_pred_proba,\n",
        "        'y_pred': y_pred\n",
        "    }\n",
        "\n",
        "# Função modificada para explicabilidade do modelo\n",
        "def model_explainability(model_results, processed_data):\n",
        "    print(\"\\n==== EXPLICABILIDADE DO MODELO (SHAP) ====\")\n",
        "\n",
        "    model = model_results['model']\n",
        "    X_new = processed_data['X_new']  # Usamos os novos dados para SHAP\n",
        "    feature_names = processed_data['feature_names']\n",
        "\n",
        "    try:\n",
        "        # Tentativa com feature_perturbation='interventional' que é mais robusto\n",
        "        explainer = shap.TreeExplainer(model, feature_perturbation='interventional')\n",
        "        shap_values = explainer.shap_values(X_new)\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao criar explicação SHAP detalhada: {e}\")\n",
        "        print(\"Tentando abordagem alternativa...\")\n",
        "\n",
        "        try:\n",
        "            # Limitando a amostra para análise SHAP\n",
        "            sample_size = min(50, X_new.shape[0])\n",
        "            X_sample = X_new.iloc[:sample_size]\n",
        "\n",
        "            # Tentar com abordagem mais simples\n",
        "            explainer = shap.TreeExplainer(model)\n",
        "            shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "            print(\"Usando amostra reduzida para análise SHAP.\")\n",
        "            X_new = X_sample  # Usar a amostra reduzida para o resto da análise\n",
        "        except Exception as e2:\n",
        "            print(f\"Não foi possível gerar explicação SHAP: {e2}\")\n",
        "            print(\"Procedendo com análise de importância de características básica.\")\n",
        "\n",
        "            # Usar apenas importância de características do Random Forest\n",
        "            feature_importance = model_results['feature_importance']\n",
        "\n",
        "            print(\"\\nImportância das características (Random Forest):\")\n",
        "            print(feature_importance.head(10))\n",
        "\n",
        "            # Retornar valores vazios\n",
        "            return None, feature_importance\n",
        "\n",
        "    # Se chegou aqui, temos valores SHAP para analisar\n",
        "    # Resumo das características\n",
        "    try:\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        shap.summary_plot(shap_values[1], X_new, feature_names=feature_names)\n",
        "        plt.title('Resumo de Impacto das Características (SHAP)')\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao criar gráfico de resumo SHAP: {e}\")\n",
        "\n",
        "    # Tentar gráficos de dependência para as características principais\n",
        "    try:\n",
        "        # Obter valores SHAP médios para cada característica\n",
        "        shap_mean_values = np.abs(shap_values[1]).mean(0)\n",
        "        top_indices = np.argsort(-shap_mean_values)[:3]  # Reduzido para top 3\n",
        "\n",
        "        # Mostrar gráficos de dependência para as top características\n",
        "        for idx in top_indices:\n",
        "            feature = feature_names[idx]\n",
        "            try:\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                shap.dependence_plot(idx, shap_values[1], X_new, feature_names=feature_names)\n",
        "                plt.title(f'Gráfico de Dependência para {feature}')\n",
        "                plt.show()\n",
        "            except Exception as e:\n",
        "                print(f\"Erro ao criar gráfico de dependência para {feature}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao calcular importância média SHAP: {e}\")\n",
        "\n",
        "    # Obter valores SHAP médios para cada característica\n",
        "    try:\n",
        "        shap_mean_values = np.abs(shap_values[1]).mean(0)\n",
        "        shap_importance = pd.DataFrame({\n",
        "            'Feature': feature_names,\n",
        "            'SHAP_Importance': shap_mean_values\n",
        "        }).sort_values('SHAP_Importance', ascending=False)\n",
        "\n",
        "        print(\"\\nImportância SHAP das características (top 10):\")\n",
        "        print(shap_importance.head(10))\n",
        "\n",
        "        return shap_values, shap_importance\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao calcular importância SHAP: {e}\")\n",
        "        # Criar um DataFrame vazio de importância\n",
        "        shap_importance = pd.DataFrame({\n",
        "            'Feature': feature_names,\n",
        "            'SHAP_Importance': np.zeros(len(feature_names))\n",
        "        })\n",
        "        return None, shap_importance\n",
        "\n",
        "# Função para identificar clientes com alto risco de churn\n",
        "def identify_high_risk(model_results, processed_data, threshold=0.7):\n",
        "    print(f\"\\n==== IDENTIFICAÇÃO DE CLIENTES COM ALTO RISCO DE CHURN (Threshold: {threshold}) ====\")\n",
        "\n",
        "    y_pred_proba = model_results['y_pred_proba']\n",
        "\n",
        "    # Usando novos dados\n",
        "    df_new = processed_data['original_df_new'].copy()\n",
        "\n",
        "    # Adicionando as probabilidades e previsões\n",
        "    df_new['churn_probability'] = y_pred_proba\n",
        "    df_new['churn_prediction'] = (y_pred_proba >= 0.5).astype(int)\n",
        "\n",
        "    # Identificando clientes de alto risco\n",
        "    high_risk = df_new[df_new['churn_probability'] >= threshold].copy()\n",
        "\n",
        "    print(f\"Total de clientes analisados: {len(df_new)}\")\n",
        "    print(f\"Total de clientes com probabilidade de churn > 50%: {(df_new['churn_prediction'] == 1).sum()} ({(df_new['churn_prediction'] == 1).mean() * 100:.2f}%)\")\n",
        "    print(f\"Total de clientes com alto risco de churn (>= {threshold*100}%): {len(high_risk)}\")\n",
        "    print(f\"Porcentagem de clientes com alto risco: {len(high_risk) / len(df_new) * 100:.2f}%\")\n",
        "\n",
        "    if len(high_risk) > 0:\n",
        "        print(\"\\nAmostra de clientes com alto risco:\")\n",
        "        display(high_risk.head().sort_values('churn_probability', ascending=False))\n",
        "\n",
        "    # Distribuição de probabilidades\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(df_new['churn_probability'], bins=20, kde=True)\n",
        "    plt.axvline(x=threshold, color='red', linestyle='--', label=f'Threshold ({threshold})')\n",
        "    plt.axvline(x=0.5, color='green', linestyle='--', label='Previsão (0.5)')\n",
        "    plt.title('Distribuição de Probabilidades de Churn')\n",
        "    plt.xlabel('Probabilidade de Churn')\n",
        "    plt.ylabel('Número de Clientes')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Criando segmentos de risco\n",
        "    def risk_category(prob):\n",
        "        if prob >= 0.8:\n",
        "            return \"Muito Alto\"\n",
        "        elif prob >= threshold:\n",
        "            return \"Alto\"\n",
        "        elif prob >= 0.5:\n",
        "            return \"Médio\"\n",
        "        elif prob >= 0.3:\n",
        "            return \"Baixo\"\n",
        "        else:\n",
        "            return \"Muito Baixo\"\n",
        "\n",
        "    df_new['risk_category'] = df_new['churn_probability'].apply(risk_category)\n",
        "\n",
        "    # Distribuição por categoria de risco\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    category_order = [\"Muito Baixo\", \"Baixo\", \"Médio\", \"Alto\", \"Muito Alto\"]\n",
        "    risk_counts = df_new['risk_category'].value_counts().reindex(category_order)\n",
        "    sns.barplot(x=risk_counts.index, y=risk_counts.values)\n",
        "    plt.title('Distribuição de Clientes por Categoria de Risco')\n",
        "    plt.xlabel('Categoria de Risco')\n",
        "    plt.ylabel('Número de Clientes')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return high_risk, df_new\n",
        "\n",
        "# Função para gerar recomendações personalizadas\n",
        "def generate_recommendations(high_risk_df, feature_importance, all_data_df):\n",
        "    print(\"\\n==== RECOMENDAÇÕES PERSONALIZADAS PARA RETENÇÃO ====\")\n",
        "\n",
        "    # Top características que influenciam o churn\n",
        "    top_features = feature_importance.head(5)['Feature'].values\n",
        "\n",
        "    print(\"As principais características que influenciam o churn são:\")\n",
        "    for i, feature in enumerate(top_features, 1):\n",
        "        print(f\"{i}. {feature}\")\n",
        "\n",
        "    # Recomendações gerais baseadas nas características mais importantes\n",
        "    print(\"\\nRecomendações gerais para reduzir churn:\")\n",
        "\n",
        "    recommendations = []\n",
        "\n",
        "    for feature in top_features:\n",
        "        if 'contract' in feature.lower():\n",
        "            rec = \"- Oferecer incentivos para contratos de longo prazo para clientes identificados com alto risco\"\n",
        "            recommendations.append({\"feature\": feature, \"recommendation\": rec})\n",
        "            print(rec)\n",
        "        elif 'tenure' in feature.lower():\n",
        "            rec = \"- Criar programas de fidelidade baseados no tempo de permanência do cliente\"\n",
        "            recommendations.append({\"feature\": feature, \"recommendation\": rec})\n",
        "            print(rec)\n",
        "        elif 'monthly' in feature.lower() and 'charges' in feature.lower():\n",
        "            rec = \"- Revisar a política de preços para clientes de alto valor com alta probabilidade de churn\"\n",
        "            recommendations.append({\"feature\": feature, \"recommendation\": rec})\n",
        "            print(rec)\n",
        "        elif 'service' in feature.lower() or 'support' in feature.lower():\n",
        "            rec = \"- Melhorar a qualidade do suporte técnico e atendimento ao cliente\"\n",
        "            recommendations.append({\"feature\": feature, \"recommendation\": rec})\n",
        "            print(rec)\n",
        "        elif 'payment' in feature.lower():\n",
        "            rec = \"- Oferecer descontos para pagamentos automáticos\"\n",
        "            recommendations.append({\"feature\": feature, \"recommendation\": rec})\n",
        "            print(rec)\n",
        "        else:\n",
        "            rec = f\"- Analisar mais profundamente como a característica '{feature}' afeta o churn\"\n",
        "            recommendations.append({\"feature\": feature, \"recommendation\": rec})\n",
        "            print(rec)\n",
        "\n",
        "    # Analisando segmentos específicos\n",
        "    print(\"\\nAnálise de segmentos específicos:\")\n",
        "\n",
        "    # Se tivermos categorias de risco, analisar por segmento\n",
        "    if 'risk_category' in all_data_df.columns:\n",
        "        high_value_customers = all_data_df[\n",
        "            (all_data_df['risk_category'].isin(['Alto', 'Muito Alto'])) &\n",
        "            (all_data_df['churn_probability'] >= 0.7)\n",
        "        ]\n",
        "\n",
        "        if len(high_value_customers) > 0:\n",
        "            print(f\"\\nIdentificados {len(high_value_customers)} clientes de alto risco que necessitam de atenção imediata\")\n",
        "\n",
        "            # Tentativa de identificar padrões comuns\n",
        "            print(\"\\nPadrões comuns entre clientes de alto risco:\")\n",
        "\n",
        "            # Para variáveis categóricas (se existirem)\n",
        "            cat_cols = high_value_customers.select_dtypes(include=['object', 'category']).columns\n",
        "            for col in cat_cols:\n",
        "                if col != 'risk_category':\n",
        "                    value_counts = high_value_customers[col].value_counts()\n",
        "                    if not value_counts.empty:\n",
        "                        top_value = value_counts.index[0]\n",
        "                        percentage = value_counts.iloc[0] / len(high_value_customers) * 100\n",
        "                        print(f\"- {col}: {percentage:.1f}% têm '{top_value}'\")\n",
        "\n",
        "    implementation_steps = [\n",
        "        \"1. Segmentar clientes de alto risco por valor e comportamento\",\n",
        "        \"2. Desenvolver ofertas personalizadas baseadas nas características específicas de cada cliente\",\n",
        "        \"3. Implementar um sistema de alerta precoce para identificar mudanças no comportamento do cliente\",\n",
        "        \"4. Criar um fluxo de trabalho automatizado para disparar ações de retenção\",\n",
        "        \"5. Monitorar e avaliar continuamente o sucesso das intervenções de retenção\"\n",
        "    ]\n",
        "\n",
        "    print(\"\\nPara implementar estas recomendações, considere:\")\n",
        "    for step in implementation_steps:\n",
        "        print(step)\n",
        "\n",
        "    return top_features, recommendations, implementation_steps\n",
        "\n",
        "# Função para exportar resultados\n",
        "def export_results(all_data_with_proba):\n",
        "    print(\"\\n==== EXPORTANDO RESULTADOS ====\")\n",
        "\n",
        "    # Criar uma cópia para exportação\n",
        "    export_df = all_data_with_proba.copy()\n",
        "\n",
        "    # Formatar as probabilidades para melhor visualização\n",
        "    export_df['churn_probability'] = export_df['churn_probability'].apply(lambda x: f\"{x:.2%}\")\n",
        "\n",
        "    # Preparar para exportação\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"previsao_churn_{timestamp}.csv\"\n",
        "\n",
        "    # Salvar no formato CSV\n",
        "    export_df.to_csv(filename, index=False)\n",
        "\n",
        "    print(f\"Arquivo de resultados salvo como: {filename}\")\n",
        "\n",
        "    # Download do arquivo\n",
        "    files.download(filename)\n",
        "\n",
        "    return filename\n",
        "\n",
        "# Função para gerar relatório com GPT-4o\n",
        "def generate_ai_report(data_dict, model_results, feature_importance, shap_importance, high_risk_data, recommendations):\n",
        "    if not openai_enabled:\n",
        "        print(\"Não foi possível gerar o relatório por IA: chave da API OpenAI não configurada.\")\n",
        "        return None\n",
        "\n",
        "    print(\"\\n==== GERANDO RELATÓRIO PERSONALIZADO COM GPT-4o ====\")\n",
        "    print(\"Aguarde enquanto o modelo de IA analisa os resultados e gera um relatório completo...\")\n",
        "\n",
        "    # Preparar os dados para enviar para a API\n",
        "    top_features = feature_importance.head(10).to_dict(orient='records')\n",
        "\n",
        "    # Usar shap_importance se disponível, caso contrário usar feature_importance\n",
        "    if shap_importance is not None and not (isinstance(shap_importance, pd.DataFrame) and 'SHAP_Importance' in shap_importance.columns and shap_importance['SHAP_Importance'].sum() > 0):\n",
        "        top_shap_features = shap_importance.head(10).to_dict(orient='records')\n",
        "    else:\n",
        "        # Se não temos valores SHAP válidos, usamos feature_importance\n",
        "        top_shap_features = feature_importance.rename(columns={'Importance': 'SHAP_Importance'}).head(10).to_dict(orient='records')\n",
        "\n",
        "    # Estatísticas sobre os dados de previsão\n",
        "    df_new = data_dict['new']\n",
        "    prediction_stats = {\n",
        "        'total_customers': len(df_new),\n",
        "        'high_risk_customers': len(high_risk_data['high_risk']),\n",
        "        'high_risk_percentage': len(high_risk_data['high_risk']) / len(df_new) * 100,\n",
        "        'avg_churn_probability': high_risk_data['all_data']['churn_probability'].mean() * 100,\n",
        "    }\n",
        "\n",
        "    # Estatísticas sobre os dados de treinamento\n",
        "    df_train = data_dict['train']\n",
        "    training_stats = {\n",
        "        'total_records': len(df_train),\n",
        "        'features_count': len(df_train.columns) - 1,  # Excluindo a coluna target\n",
        "        'churn_rate': df_train['Churn'].mean() * 100\n",
        "    }\n",
        "\n",
        "    # Criar um prompt para o GPT-4o\n",
        "    prompt = f\"\"\"\n",
        "    Você é um consultor especialista em análise de negócios e retenção de clientes. Um sistema de previsão de churn foi executado e gerou os seguintes resultados:\n",
        "\n",
        "    # DADOS DE TREINAMENTO\n",
        "    - Total de registros no conjunto de treinamento: {training_stats['total_records']}\n",
        "    - Número de características usadas: {training_stats['features_count']}\n",
        "    - Taxa de churn histórica: {training_stats['churn_rate']:.2f}%\n",
        "\n",
        "    # RESULTADOS DA PREVISÃO\n",
        "    - Total de clientes analisados: {prediction_stats['total_customers']}\n",
        "    - Clientes identificados com alto risco de churn: {prediction_stats['high_risk_customers']}\n",
        "    - Porcentagem de clientes com alto risco: {prediction_stats['high_risk_percentage']:.2f}%\n",
        "    - Probabilidade média de churn: {prediction_stats['avg_churn_probability']:.2f}%\n",
        "\n",
        "    # IMPORTÂNCIA DAS CARACTERÍSTICAS (RANDOM FOREST)\n",
        "    {json.dumps(top_features, indent=2)}\n",
        "\n",
        "    # IMPORTÂNCIA SHAP (EXPLAINABILITY)\n",
        "    {json.dumps(top_shap_features, indent=2)}\n",
        "\n",
        "    # RECOMENDAÇÕES PARA RETENÇÃO\n",
        "    {json.dumps(recommendations, indent=2)}\n",
        "\n",
        "    Com base nesses dados, elabore um relatório executivo completo e estruturado que:\n",
        "    1. Resuma os principais insights da análise\n",
        "    2. Identifique os principais fatores que contribuem para o churn\n",
        "    3. Apresente um plano estratégico detalhado para reduzir a taxa de churn\n",
        "    4. Forneça recomendações específicas para diferentes segmentos de clientes\n",
        "    5. Sugira métricas de acompanhamento para medir o sucesso das intervenções\n",
        "    6. Inclua próximos passos e melhorias futuras para o sistema\n",
        "\n",
        "    O relatório deve ser bem estruturado com títulos e subtítulos, ter tom profissional e ser direcionado para executivos de negócios. Inclua insights que não são óbvios a partir dos dados puros e recomendações que sejam acionáveis.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Chamar a API da OpenAI\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Você é um consultor especialista em análise de dados e retenção de clientes, capaz de transformar dados técnicos em insights acionáveis de negócios.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.7,\n",
        "            max_tokens=4000\n",
        "        )\n",
        "\n",
        "        # Extrair o relatório da resposta\n",
        "        report = response.choices[0].message.content\n",
        "\n",
        "        # Exibir o relatório\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"RELATÓRIO EXECUTIVO GERADO PELA IA\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "        print(report)\n",
        "\n",
        "        # Salvar o relatório em um arquivo\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"relatorio_churn_{timestamp}.md\"\n",
        "\n",
        "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(report)\n",
        "\n",
        "        print(f\"\\nRelatório salvo em: {filename}\")\n",
        "\n",
        "        # Download do relatório\n",
        "        files.download(filename)\n",
        "\n",
        "        return report\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao gerar relatório com GPT-4o: {e}\")\n",
        "        return None\n",
        "\n",
        "# Função principal\n",
        "def main():\n",
        "    print(\"=\"*70)\n",
        "    print(\"SISTEMA DE PREVISÃO DE CHURN DE CLIENTES - MODO PREVISÃO\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nEste sistema treina um modelo usando dados históricos e faz previsões\")\n",
        "    print(\"de churn em um novo conjunto de clientes para identificar aqueles com maior risco.\")\n",
        "\n",
        "    # Upload dos dados (treino e previsão)\n",
        "    data_dict = upload_data()\n",
        "\n",
        "    # Análise rápida dos dados de treinamento\n",
        "    data_dict['train'] = analyze_training_data(data_dict['train'])\n",
        "\n",
        "    # Pré-processamento\n",
        "    processed_data = preprocess_data(data_dict)\n",
        "\n",
        "    if processed_data is not None:\n",
        "        # Treinamento e previsão\n",
        "        model_results = train_and_predict(processed_data)\n",
        "\n",
        "        # Explicabilidade do modelo (versão robusta)\n",
        "        shap_values, shap_importance = model_explainability(model_results, processed_data)\n",
        "\n",
        "        # Verificar se temos valores SHAP válidos\n",
        "        if shap_values is None:\n",
        "            print(\"Aviso: A análise SHAP completa não foi possível. Usando apenas importância de características do modelo.\")\n",
        "            # Use feature_importance como substituto\n",
        "            shap_importance = model_results['feature_importance'].rename(columns={'Importance': 'SHAP_Importance'})\n",
        "\n",
        "        # Identificação de clientes com alto risco\n",
        "        high_risk, all_data_with_proba = identify_high_risk(model_results, processed_data)\n",
        "\n",
        "        high_risk_data = {\n",
        "            'high_risk': high_risk,\n",
        "            'all_data': all_data_with_proba\n",
        "        }\n",
        "\n",
        "        # Geração de recomendações\n",
        "        top_features, recommendations_list, implementation_steps = generate_recommendations(\n",
        "            high_risk, model_results['feature_importance'], all_data_with_proba\n",
        "        )\n",
        "\n",
        "        # Exportar resultados\n",
        "        export_filename = export_results(all_data_with_proba)\n",
        "\n",
        "        # Perguntar se o usuário deseja gerar um relatório com IA\n",
        "        if openai_enabled:\n",
        "            generate_report = input(\"\\nDeseja gerar um relatório executivo personalizado usando GPT-4o? (s/n): \").strip().lower()\n",
        "\n",
        "            if generate_report == 's':\n",
        "                # Gerar relatório com IA\n",
        "                ai_report = generate_ai_report(\n",
        "                    data_dict,\n",
        "                    model_results,\n",
        "                    model_results['feature_importance'],\n",
        "                    shap_importance,\n",
        "                    high_risk_data,\n",
        "                    {'recommendations': recommendations_list, 'implementation_steps': implementation_steps}\n",
        "                )\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"CONCLUSÃO\")\n",
        "        print(\"=\"*70)\n",
        "        print(\"O sistema de previsão de churn foi executado com sucesso.\")\n",
        "        print(f\"\\nTotal de clientes analisados: {len(all_data_with_proba)}\")\n",
        "        print(f\"Clientes com alto risco de churn: {len(high_risk)} ({len(high_risk)/len(all_data_with_proba)*100:.2f}%)\")\n",
        "\n",
        "        print(\"\\nOs resultados detalhados foram exportados para:\")\n",
        "        print(f\"- {export_filename}\")\n",
        "\n",
        "        if openai_enabled and 'generate_report' in locals() and generate_report == 's':\n",
        "            print(\"- O relatório executivo gerado pela IA também foi salvo e disponibilizado para download\")\n",
        "\n",
        "        print(\"\\nUse estes resultados para implementar estratégias de retenção direcionadas\")\n",
        "        print(\"aos clientes com maior risco de cancelamento.\")\n",
        "        print(\"\\nObrigado por usar o Sistema de Previsão de Churn!\")\n",
        "\n",
        "# Executar o sistema\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}